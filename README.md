# ğŸ¬ Real-Time Movie Recommendation System
## Production-Ready Lambda Architecture with Airflow 3.x + Spark 3.5 + Kafka


A complete, production-ready movie recommendation system implementing Lambda Architecture for both real-time and batch processing.

---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Architecture](#-architecture)
- [Quick Start](#-quick-start)
- [Project Structure](#-project-structure)
- [Configuration](#-configuration)
- [Usage Examples](#-usage-examples)
- [Monitoring](#-monitoring)
- [Troubleshooting](#-troubleshooting)
- [Performance Tuning](#-performance-tuning)
- [Contributing](#-contributing)

---

## âœ¨ Features

### Real-Time Layer (Speed Layer)
- âš¡ **Sub-second latency**: User interactions processed in < 500ms
- ğŸ¯ **Instant recommendations**: Updates recommendations within seconds of user action
- ğŸ“Š **Event streaming**: Kafka-based event processing with exactly-once semantics
- ğŸ’¾ **Multi-tier caching**: Redis (L1) â†’ Postgres (L2) â†’ Batch (L3)

### Batch Layer
- ğŸ”„ **Daily model training**: Collaborative filtering with ALS (Alternating Least Squares)
- ğŸ“ˆ **Historical analysis**: Process full user interaction history
- ğŸ“ **ML pipeline**: Complete MLOps workflow with model versioning
- ğŸ—„ï¸ **Data archival**: Automatic archival of old events to object storage

### Infrastructure
- ğŸ³ **Fully containerized**: Docker Compose orchestration
- ğŸ—ï¸ **Production-ready**: Multi-architecture support (ARM64 + AMD64)
- ğŸ“Š **Monitoring-ready**: Health checks, metrics endpoints
- ğŸ”’ **Security-hardened**: RBAC, secrets management, network isolation

---

## ğŸ—ï¸ Architecture

### Full System Diagram
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         USER'S BROWSER                          â”‚
â”‚                      http://localhost:3000                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  React Application (Frontend)                                   â”‚
â”‚  â”œâ”€ Components: RecommendationPanel, MovieCard, Dashboard      â”‚
â”‚  â”œâ”€ State: userId, sessionId, recommendations[]                â”‚
â”‚  â””â”€ API Client: Fetch to http://localhost/api/*                â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ HTTP Requests
                     â”‚ (CORS: localhost:3000 allowed)
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸŒ NGINX API GATEWAY                         â”‚
â”‚                      http://localhost/ (Port 80)                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  ROUTING RULES:                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Pattern         Backend        Purpose                    â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ /api/*       â†’  FastAPI:8000   Main API                  â”‚ â”‚
â”‚  â”‚ /admin/*     â†’  Airflow:8080   Workflow UI               â”‚ â”‚
â”‚  â”‚ /storage/*   â†’  MinIO:9001     File storage              â”‚ â”‚
â”‚  â”‚ /docs        â†’  FastAPI:8000   API documentation         â”‚ â”‚
â”‚  â”‚ /health      â†’  Gateway        Health check              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                 â”‚
â”‚  FEATURES:                                                      â”‚
â”‚  â”œâ”€ ğŸ›¡ï¸ Rate Limiting:    100 req/min per IP                   â”‚
â”‚  â”œâ”€ ğŸ”’ CORS Handling:    Centralized policy                   â”‚
â”‚  â”œâ”€ ğŸ“ Access Logging:   Detailed request logs                â”‚
â”‚  â”œâ”€ ğŸ” Security Headers: X-Frame-Options, CSP, etc.           â”‚
â”‚  â”œâ”€ ğŸ“¦ Compression:      gzip (90% size reduction)            â”‚
â”‚  â”œâ”€ âš¡ Connection Pool:  Keepalive to backends                â”‚
â”‚  â””â”€ ğŸ¯ Load Balancing:   (Future: Multiple FastAPI instances) â”‚
â”‚                                                                 â”‚
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚          â”‚          â”‚
   â”‚          â”‚          â”‚ Internal Network (Docker)
   â”‚          â”‚          â”‚
   â†“          â†“          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BACKEND SERVICES                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  FastAPI Application Server (Port 8000)                    â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚  Endpoints:                                                 â”‚ â”‚
â”‚  â”‚  â”œâ”€ GET  /api/recommendations  (Hybrid L2+L3)              â”‚ â”‚
â”‚  â”‚  â”œâ”€ POST /api/events           (Kafka â†’ Spark)             â”‚ â”‚
â”‚  â”‚  â”œâ”€ GET  /api/movies            (Browse catalog)           â”‚ â”‚
â”‚  â”‚  â”œâ”€ GET  /api/metrics           (CTR, cache stats)         â”‚ â”‚
â”‚  â”‚  â””â”€ GET  /health                (Service status)           â”‚ â”‚
â”‚  â”‚                                                             â”‚ â”‚
â”‚  â”‚  Logic:                                                     â”‚ â”‚
â”‚  â”‚  â”œâ”€ L1 Cache Check (Redis)                                 â”‚ â”‚
â”‚  â”‚  â”œâ”€ L2 Realtime Compute (pgvector)                         â”‚ â”‚
â”‚  â”‚  â”œâ”€ L3 Batch Fallback (pre-computed)                       â”‚ â”‚
â”‚  â”‚  â””â”€ Hybrid Blending (70% batch + 30% realtime)            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                  â†“                  â†“                  â†“         â”‚
â”‚                  â”‚                  â”‚                  â”‚         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚               â†“                  â†“                  â†“      â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚  â”‚   Redis      â”‚  â”‚  Postgres    â”‚  â”‚    Kafka     â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  Port 6379   â”‚  â”‚  Port 5432   â”‚  â”‚  Port 29092  â”‚   â”‚ â”‚
â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚   â”‚ â”‚
â”‚  â”‚  â”‚ L1 Cache     â”‚  â”‚ Tables:      â”‚  â”‚ Topics:      â”‚   â”‚ â”‚
â”‚  â”‚  â”‚ â”œâ”€ recs:*    â”‚  â”‚ â”œâ”€ movies    â”‚  â”‚ â”œâ”€ movie-evtsâ”‚   â”‚ â”‚
â”‚  â”‚  â”‚ â””â”€ TTL: 1h   â”‚  â”‚ â”œâ”€ user_prof â”‚  â”‚ â””â”€ Consumers:â”‚   â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚ â”œâ”€ batch_recsâ”‚  â”‚    Spark     â”‚   â”‚ â”‚
â”‚  â”‚  â”‚ Hit Rate:    â”‚  â”‚ â”œâ”€ rec_logs  â”‚  â”‚              â”‚   â”‚ â”‚
â”‚  â”‚  â”‚ ~85%         â”‚  â”‚ â””â”€ rec_feed  â”‚  â”‚ Retention:   â”‚   â”‚ â”‚
â”‚  â”‚  â”‚              â”‚  â”‚              â”‚  â”‚ 7 days       â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â”‚                            â†‘                  â†‘           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                â”‚                  â”‚             â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚              â”‚                          â”‚                       â”‚
â”‚              â†“                          â†“                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  Apache Spark          â”‚  â”‚  Apache Airflow        â”‚        â”‚
â”‚  â”‚  (Master + Worker)     â”‚  â”‚  (Scheduler + Web)     â”‚        â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”‚
â”‚  â”‚                        â”‚  â”‚                        â”‚        â”‚
â”‚  â”‚  Real-time Layer:      â”‚  â”‚  Batch Layer:          â”‚        â”‚
â”‚  â”‚  â”œâ”€ Streaming Job      â”‚  â”‚  â”œâ”€ DAG: batch_recs   â”‚        â”‚
â”‚  â”‚  â”‚  (Kafka consumer)   â”‚  â”‚  â”‚  Schedule: 2 AM    â”‚        â”‚
â”‚  â”‚  â”‚  Interval: 5s       â”‚  â”‚  â”‚                    â”‚        â”‚
â”‚  â”‚  â”‚                     â”‚  â”‚  â”‚  Tasks:            â”‚        â”‚
â”‚  â”‚  â”‚  Updates:           â”‚  â”‚  â”‚  1. Archive events â”‚        â”‚
â”‚  â”‚  â”‚  2. Train ALS modelâ”‚        â”‚
â”‚  â”‚  â”‚                     â”‚  â”‚  â”‚  3. Generate recs  â”‚        â”‚
â”‚  â”‚  â”‚                     â”‚  â”‚  â”‚  4. Atomic swap    â”‚        â”‚
â”‚  â”‚  Batch Layer:          â”‚  â”‚  â”‚  5. Invalidate L1  â”‚        â”‚
â”‚  â”‚  â””â”€ ALS Training       â”‚  â”‚  â”‚                    â”‚        â”‚
â”‚     (nightly, triggered   â”‚  â”‚  â””â”€ Port: 8080        â”‚        â”‚
â”‚      by Airflow)          â”‚  â”‚                        â”‚        â”‚
â”‚  â”‚                        â”‚  â”‚  Accessed via:         â”‚        â”‚
â”‚  â”‚  Port: 7077 (master)   â”‚  â”‚  http://localhost/     â”‚        â”‚
â”‚  â”‚  Port: 8081 (worker)   â”‚  â”‚         admin/         â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚              â†“                                                  â”‚
â”‚              â†“                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                    MinIO Object Storage                â”‚    â”‚
â”‚  â”‚                   Port 9000 (API)                      â”‚    â”‚
â”‚  â”‚                   Port 9001 (Console)                  â”‚    â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â”‚  Buckets:                                              â”‚    â”‚
â”‚  â”‚  â”œâ”€ lakehouse/                                         â”‚    â”‚
â”‚  â”‚  â”‚  â”œâ”€ models/                (Trained ALS models)    â”‚    â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€ als_20250103_02am/                          â”‚    â”‚
â”‚  â”‚  â”‚  â”œâ”€ archived-events/        (Historical data)      â”‚    â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€ 2025/01/03/events.parquet                   â”‚    â”‚
â”‚  â”‚  â”‚  â”œâ”€ batch-recs-history/     (Daily snapshots)      â”‚    â”‚
â”‚  â”‚  â”‚  â””â”€ checkpoints/            (Spark state)          â”‚    â”‚
â”‚  â”‚  â”‚                                                     â”‚    â”‚
â”‚  â”‚  Accessed via:                                         â”‚    â”‚
â”‚  â”‚  â”œâ”€ Internal: s3a://lakehouse/                        â”‚    â”‚
â”‚  â”‚  â””â”€ External: http://localhost/storage/               â”‚    â”‚
â”‚  â”‚                                                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Request Flow with Gateway

**Example: Get Recommendations**

`TIMELINE: User requests personalized recommendations`

1.  **T=0ms | User's Browser**: `fetch('http://localhost/api/recommendations?user_id=user_123')`
2.  **T=5ms | Nginx Gateway (Port 80)**:
    *   CORS Check: Origin localhost:3000 â†’ âœ… Allowed
    *   Rate Limit: IP 192.168.1.100 â†’ 45/100 requests â†’ âœ… OK
    *   Route Match: `/api/*` â†’ FastAPI backend
    *   Proxy: `http://fastapi:8000/api/recommendations`
3.  **T=10ms | FastAPI (Port 8000)**:
    *   Check L1 Cache (Redis) â†’ MISS
    *   Get L2 Realtime Recs (pgvector)
    *   Get L3 Batch Recs (Postgres)
    *   Blend Hybrid (70% batch + 30% realtime)
    *   Cache Result in Redis
    *   Log to recommendation_logs
4.  **T=100ms | FastAPI Response**:
    *   Returns JSON with recommendations and metadata.
5.  **T=105ms | Nginx Gateway**:
    *   Adds Security Headers
    *   Compresses Response (gzip)
    *   Logs Request
6.  **T=110ms | User's Browser**:
    *   Response received and UI renders.

### Gateway Benefits Illustrated

**Without Gateway (Before)**:
*   âŒ Frontend knows all backend URLs
*   âŒ Each service handles CORS independently
*   âŒ No centralized rate limiting
*   âŒ Security risk (all ports exposed)

**With Gateway (After)**:
*   âœ… Single URL (http://localhost)
*   âœ… Rate limiting & Logging
*   âœ… CORS centralized
*   âœ… Security headers & Compression

### Port Mapping with Gateway

| Port | Service | Accessible From | Purpose |
|------|---------|-----------------|---------|
| 80 | Nginx Gateway | External | **MAIN ENTRY POINT** |
| 3000 | React Frontend | External | User Interface |
| 8000 | FastAPI | Internal | API (via gateway) |
| 8080 | Airflow Web | Internal | Workflow UI (via gateway) |
| 9000 | MinIO API | Internal | Object storage (via gateway) |
| 9001 | MinIO Console | Internal | Storage UI (via gateway) |
| 5432 | Postgres | Internal | Database |
| 6379 | Redis | Internal | Cache |
| 29092| Kafka | Internal | Event stream |

### Security Layers

1.  **Layer 1: Network Isolation**: Frontend (Public) â†’ Gateway (Bridge) â†’ Backend (Private)
2.  **Layer 2: Gateway Policies**: Rate Limiting, CORS, Headers, Request Size
3.  **Layer 3: Application Security**: Input Validation, SQL Injection Protection
4.  **Layer 4: Data Security**: Password Auth, Access Keys

---

## ğŸš€ Quick Start

### Prerequisites

```bash
# Required
âœ… Docker Desktop (with 8GB RAM allocated)
âœ… Docker Compose V2+
âœ… 20GB+ free disk space
âœ… Mac M4 (ARM64) or Intel/AMD (AMD64)

# Verify installations
docker --version          # Should be 20.x+
docker compose version    # Should be 2.x+
```

### Installation (5 Minutes)

**Step 1: Clone Repository**
```bash
git clone <repository-url>
cd movie-recommendation-system
```

**Step 2: Configure Environment**
```bash
# Copy environment template
cp .env.example .env

# Edit .env file (optional, defaults work for local development)
vim .env
```

**Step 3: Start All Services**
```bash
# Start infrastructure
docker compose up -d

# Watch logs (optional)
docker compose logs -f
```

**Step 4: Wait for Health Checks** (2-3 minutes)
```bash
# Check service status
docker compose ps

# All services should show "healthy" status
```

**Step 5: Access UIs**

Open in browser:
```
ğŸŒ Frontend App:     http://localhost:3000
ğŸŒ API Gateway:      http://localhost/
   â”œâ”€ Docs:          http://localhost/docs
   â”œâ”€ Airflow:       http://localhost/admin
   â””â”€ MinIO:         http://localhost/storage
```

### Verify Installation

```bash
# Test API Gateway health
curl http://localhost/health

# Expected response:
# Gateway OK

# Test recommendation endpoint via Gateway
curl "http://localhost/api/recommendations?user_id=test_user_1&limit=5"
```

---

## ğŸ“ Project Structure

```
movie-recommendation-system/
â”‚
â”œâ”€â”€ docker-compose.yaml              # Main orchestration file
â”œâ”€â”€ .env.example                     # Environment variables template
â”œâ”€â”€ README.md                        # This file
â”‚
â”œâ”€â”€ api-gateway/                     # NGINX API Gateway
â”‚   â””â”€â”€ nginx.conf                   # Routing and security config
â”‚
â”œâ”€â”€ frontend/                        # React Application
â”‚   â”œâ”€â”€ src/                         # Source code
â”‚   â””â”€â”€ Dockerfile                   # Frontend container
â”‚
â”œâ”€â”€ airflow/                         # Airflow 3.1.5 configuration
â”‚   â”œâ”€â”€ Dockerfile                   # Custom Airflow image
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ airflow.cfg              # Optional custom config
â”‚
â”œâ”€â”€ dags/                            # Airflow DAGs
â”‚   â”œâ”€â”€ batch_recommendation_dag.py  # Daily batch processing
â”‚   â””â”€â”€ model_training_dag.py        # ML model training
â”‚
â”œâ”€â”€ spark/                           # Spark 3.5.3 configuration
â”‚   â””â”€â”€ Dockerfile                   # Custom Spark image
â”‚
â”œâ”€â”€ spark-jobs/                      # Spark applications
â”‚   â”œâ”€â”€ streaming_recommendations.py # Real-time processing
â”‚   â””â”€â”€ batch_model_training.py      # Batch ML training
â”‚
â”œâ”€â”€ fastapi/                         # API application
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ main.py                  # FastAPI application
â”‚       â”œâ”€â”€ config.py                # Configuration
â”‚       â””â”€â”€ models.py                # Pydantic models
â”‚
â”œâ”€â”€ init-scripts/                    # Database initialization
â”‚   â””â”€â”€ init-pgvector.sql            # PostgreSQL schema
â”‚
â””â”€â”€ docs/                            # Documentation
    â”œâ”€â”€ QUICK_START_GUIDE.md
    â”œâ”€â”€ AIRFLOW_3X_MIGRATION_GUIDE.md
    â””â”€â”€ DOCKERFILE_FIXES_CHANGELOG.md
```

---

## âš™ï¸ Configuration

### Resource Allocation (8GB Docker Memory)

Default allocation optimized for Mac M4 16GB (8GB Docker limit):

| Service          | Memory | CPUs | Purpose                    |
|------------------|--------|------|----------------------------|
| Spark Worker     | 1.5GB  | 2.0  | Data processing            |
| Kafka            | 768MB  | 1.0  | Message broker             |
| Postgres         | 512MB  | 0.5  | Database                   |
| MinIO            | 512MB  | 0.5  | Object storage             |
| Airflow Web      | 512MB  | 0.5  | UI                         |
| Airflow Scheduler| 512MB  | 0.5  | Orchestration              |
| Redis            | 256MB  | 0.25 | Cache                      |
| FastAPI          | 256MB  | 0.5  | API                        |
| Spark Master     | 256MB  | 0.5  | Coordinator                |
| Zookeeper        | 256MB  | 0.25 | Kafka coordination         |
| Nginx Gateway    | 128MB  | 0.25 | API Gateway                |
| Frontend         | 128MB  | 0.25 | UI                         |

**Total: ~5.6GB** (2.4GB buffer for system)

### Environment Variables

Key variables in `.env`:

```bash
# Performance tuning
AIRFLOW__CORE__PARALLELISM=32              # Concurrent tasks
SPARK_WORKER_MEMORY=1536m                  # Spark worker RAM
POSTGRES_MAX_CONNECTIONS=150               # DB connections

# Security (CHANGE IN PRODUCTION!)
AIRFLOW__CORE__FERNET_KEY=<generate-new>
POSTGRES_PASSWORD=<strong-password>
MINIO_ROOT_PASSWORD=<strong-password>

# Application settings
RECOMMENDATION_CACHE_TTL=3600              # Cache TTL (seconds)
SPARK_STREAMING_BATCH_INTERVAL=5           # Micro-batch interval
```

See `.env.example` for full list of options.

---

## ğŸ“– Usage Examples

### Send User Interaction Event

```bash
# User clicks on "Inception"
curl -X POST http://localhost/api/events \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user_123",
    "movie_id": "m002",
    "event_type": "click"
  }'

# Response: {"status":"accepted","message":"Event queued for processing"}
```

### Get Recommendations

```bash
# Get top 10 recommendations for user
curl "http://localhost/api/recommendations?user_id=user_123&limit=10"

# Response:
# {
#   "user_id": "user_123",
#   "recommendations": [
#     {"movie_id":"m003","title":"Interstellar","score":0.94},
#     ...
#   ],
#   "source": "redis_cache",
#   "generated_at": "2025-01-01T00:00:00Z"
# }
```

### Start Spark Streaming Job

```bash
# Submit streaming job to Spark cluster
docker exec -it spark-master \
  /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3 \
  /opt/spark-jobs/streaming_recommendations.py

# Monitor in Spark UI: http://localhost:8088
```

### Trigger Airflow DAG

```bash
# Via CLI
docker exec -it airflow-webserver \
  airflow dags trigger batch_recommendation_processing

# Or via UI: http://localhost/admin
# Navigate to DAGs â†’ batch_recommendation_processing â†’ Trigger DAG
```

---

## ğŸ“Š Monitoring

### Health Checks

```bash
# All services health
./scripts/health_check.sh

# Individual service health
curl http://localhost/health          # Gateway
curl http://localhost/api/health      # FastAPI (via Gateway)
curl http://localhost/admin/health    # Airflow (via Gateway)
```

### Metrics

```bash
# FastAPI metrics
curl http://localhost/api/metrics

# Example response:
# {
#   "total_requests": 15234,
#   "cache_hit_rate": 0.87,
#   "avg_response_time_ms": 12.3
# }
```

### Logs

```bash
# View all logs
docker compose logs -f

# Service-specific logs
docker compose logs -f fastapi
docker compose logs -f airflow-scheduler
docker compose logs -f spark-worker
docker compose logs -f nginx
docker compose logs -f frontend

# Airflow task logs
docker exec airflow-webserver \
  airflow tasks logs batch_recommendation_processing <task_id> <execution_date>
```

---

## ğŸ”§ Troubleshooting

### Services Won't Start

```bash
# Check Docker resources
docker system df
docker stats

# Clean up
docker system prune -f
docker volume prune -f

# Restart services
docker compose down
docker compose up -d
```

### Health Checks Failing

**Problem**: Container marked as "unhealthy"

```bash
# Check specific service logs
docker logs kafka

# Common issues:
# 1. Kafka: Wait 30+ seconds for full startup
# 2. Postgres: Database still initializing
# 3. Airflow: DB migration in progress

# Solution: Wait and check again
docker compose ps
```

### Out of Memory Errors

**Problem**: `OOMKilled` or `java.lang.OutOfMemoryError`

```bash
# Check Docker memory limit
docker info | grep Memory

# Solution 1: Increase Docker memory (Docker Desktop â†’ Settings)
# Solution 2: Reduce memory allocation in docker-compose.yaml
```

### Spark Job Failures

**Problem**: Spark jobs fail with connection errors

```bash
# Verify Spark cluster health
curl http://localhost:8088

# Check if worker is registered
# UI should show 1 worker with 2 cores, 1.5GB memory

# Common issues:
# 1. JAVA_HOME not set â†’ Check Spark Dockerfile
# 2. JAR conflicts â†’ Verify hadoop-aws versions match
# 3. Memory exceeded â†’ Reduce executor memory

# Test Spark submit
docker exec spark-master \
  /opt/spark/bin/spark-submit \
  --version
```

### Database Connection Issues

```bash
# Test Postgres connection
docker exec -it postgres \
  psql -U airflow -d moviedb -c "SELECT COUNT(*) FROM movies;"

# Test from FastAPI container
docker exec -it fastapi \
  python -c "import asyncpg; import asyncio; asyncio.run(asyncpg.connect('postgresql://airflow:airflow@postgres:5432/moviedb'))"
```

---

## ğŸ¯ Performance Tuning

### For Higher Throughput

```yaml
# In .env or docker-compose.yaml
environment:
  # Increase parallelism
  - AIRFLOW__CORE__PARALLELISM=64
  - AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY=512
  
  # More Spark resources
  - SPARK_WORKER_MEMORY=4g
  - SPARK_WORKER_CORES=4
  
  # More database connections
  - POSTGRES_MAX_CONNECTIONS=300
  - AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE=20
```

### For Lower Latency

```yaml
# Reduce batch intervals
environment:
  - SPARK_STREAMING_BATCH_INTERVAL=2  # From 5s to 2s
  
# More aggressive caching
  - RECOMMENDATION_CACHE_TTL=7200  # 2 hours
  
# Increase Redis memory
  - REDIS_MAX_MEMORY=512mb
```

### For Better Accuracy

```yaml
# Longer model training
environment:
  - ALS_MAX_ITER=20        # From 10 to 20
  - ALS_RANK=20            # More latent factors
  - RECOMMENDATION_TOP_N=100  # Generate more candidates
```

---

## ğŸ“ Development

### Running Tests

```bash
# Unit tests
docker exec fastapi pytest tests/

# Integration tests
docker exec airflow-webserver \
  airflow dags test batch_recommendation_processing 2025-01-01
```

### Hot Reload (Development Mode)

FastAPI and Airflow DAGs support hot reload:

```bash
# Edit files locally
vim fastapi/app/main.py
vim dags/batch_recommendation_dag.py

# Changes apply immediately (mounted volumes)
# No need to rebuild containers
```

### Rebuilding After Code Changes

```bash
# Rebuild specific service
docker compose build airflow-webserver
docker compose up -d airflow-webserver

# Rebuild all
docker compose build
docker compose up -d
```

---

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details

---

## ğŸ™ Acknowledgments

- Apache Airflow team for the excellent 3.x release
- Apache Spark community
- pgvector maintainers
- FastAPI creator SebastiÃ¡n RamÃ­rez

---

## ğŸ“š Additional Documentation

- [Airflow 3.x Migration Guide](docs/AIRFLOW_3X_MIGRATION_GUIDE.md)
- [Dockerfile Fixes Changelog](docs/DOCKERFILE_FIXES_CHANGELOG.md)
- [Quick Start Guide](docs/QUICK_START_GUIDE.md)

---

**Built with â¤ï¸ for production ML systems**