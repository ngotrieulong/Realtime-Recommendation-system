# ğŸ”„ AIRFLOW 2.x â†’ 3.x MIGRATION GUIDE

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸ“‹ TÃ“M Táº®T - Nhá»¯ng gÃ¬ thay Ä‘á»•i

Anh Long Æ¡i, Ä‘Ã¢y lÃ  nhá»¯ng thay Ä‘á»•i QUAN TRá»ŒNG khi upgrade tá»« Airflow 2.8.1 â†’ 3.1.5:

### âœ… GOOD NEWS - Mostly Backward Compatible

Airflow team Ä‘Ã£ ráº¥t cáº©n tháº­n vá»›i breaking changes. Äa sá»‘ DAGs tá»« 2.x sáº½ cháº¡y OK trÃªn 3.x vá»›i minimal modifications. NhÆ°ng váº«n cÃ³ má»™t sá»‘ Ä‘iá»ƒm cáº§n update.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸ”§ PART 1: DOCKER-COMPOSE CHANGES

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### 1.1 Environment Variables Updated

```yaml
# âŒ REMOVED (no longer needed in Airflow 3.x):
- AIRFLOW__WEBSERVER__RBAC=True  # RBAC is MANDATORY in 3.x
- AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True  # Default changed to False

# âœ… KEEP (still valid):
- AIRFLOW__CORE__EXECUTOR=LocalExecutor
- AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://...
- AIRFLOW__CORE__FERNET_KEY=...
- AIRFLOW__CORE__LOAD_EXAMPLES=False
- AIRFLOW__CORE__DEFAULT_TIMEZONE=Asia/Ho_Chi_Minh

# âœ¨ NEW (recommended to add):
- AIRFLOW__WEBSERVER__EXPOSE_CONFIG=False        # Security
- AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME=False      # Security
- AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE=False    # Security
- AIRFLOW__SCHEDULER__USE_JOB_SCHEDULE=True      # Performance
```

### 1.2 Updated Docker Compose Section

Replace current airflow-webserver section vá»›i:

```yaml
  airflow-webserver:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-webserver
    hostname: airflow-webserver
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      # Core configuration
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__DEFAULT_TIMEZONE=Asia/Ho_Chi_Minh
      
      # Connections (unchanged)
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
      - AIRFLOW_CONN_AWS_DEFAULT=aws://minioadmin:minioadmin@?host=http://minio:9000&region_name=us-east-1
      
      # Security settings (Airflow 3.x enhanced)
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=False
      - AIRFLOW__WEBSERVER__EXPOSE_HOSTNAME=False
      - AIRFLOW__WEBSERVER__EXPOSE_STACKTRACE=False
      
      # Performance tuning (Airflow 3.x)
      - AIRFLOW__CORE__PARALLELISM=32
      - AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=16
      
      # Auto-initialization (unchanged)
      - _AIRFLOW_DB_UPGRADE=true
      - _AIRFLOW_WWW_USER_CREATE=true
      - _AIRFLOW_WWW_USER_USERNAME=admin
      - _AIRFLOW_WWW_USER_PASSWORD=admin
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./spark-jobs:/opt/spark-jobs:ro
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/plugins:/opt/airflow/plugins
    networks:
      - lakehouse-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 90s  # Increased from 60s (3.x needs more time)
    command: webserver
    restart: unless-stopped
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸ PART 2: DAG CODE CHANGES

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### 2.1 Removed Operators (Breaking Changes)

```python
# âŒ REMOVED - Will cause ImportError:
from airflow.operators.subdag import SubDagOperator  
from airflow.operators.python import PythonVirtualenvOperator  # Old API

# âœ… REPLACEMENTS:
from airflow.utils.task_group import TaskGroup  # Instead of SubDag
from airflow.decorators import task  # Use @task.virtualenv decorator
```

**Migration Example**:

```python
# BEFORE (Airflow 2.x) - Using SubDagOperator:
from airflow.operators.subdag import SubDagOperator

subdag_task = SubDagOperator(
    task_id='subdag_task',
    subdag=create_subdag(...),
    dag=dag
)

# AFTER (Airflow 3.x) - Using TaskGroup:
from airflow.utils.task_group import TaskGroup

with TaskGroup(group_id='task_group') as tg:
    task1 = BashOperator(...)
    task2 = PythonOperator(...)
    task1 >> task2
```

### 2.2 Execution Date â†’ Logical Date

```python
# âŒ DEPRECATED (still works but warning):
execution_date = context['execution_date']

# âœ… NEW API:
logical_date = context['logical_date']

# Or use data_interval_start / data_interval_end:
data_interval_start = context['data_interval_start']
data_interval_end = context['data_interval_end']
```

**Why this change?**
- "Execution date" was confusing (it's not actually when task executes!)
- "Logical date" better represents the data being processed
- Clearer separation between schedule time vs execution time

### 2.3 DAG Definition Best Practices

```python
# âŒ OLD STYLE (2.x):
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'retries': 1,
}

dag = DAG(
    'my_dag',
    default_args=default_args,
    schedule_interval='@daily',
    catchup=False
)

# âœ… NEW STYLE (3.x - recommended):
from airflow.models.dag import DAG

dag = DAG(
    dag_id='my_dag',
    schedule='@daily',  # 'schedule' instead of 'schedule_interval'
    catchup=False,
    # Pass args directly (cleaner)
    owner='airflow',
    depends_on_past=False,
    email_on_failure=False,
    retries=1,
    tags=['production', 'batch'],  # Better organization
)
```

**Note**: `schedule_interval` still works, but `schedule` is preferred.

### 2.4 XCom Pull/Push Changes

```python
# âŒ OLD (loose typing):
value = task_instance.xcom_pull(task_ids='upstream_task')

# âœ… NEW (stricter typing - recommended):
from typing import Any

value: Any = task_instance.xcom_pull(
    task_ids='upstream_task',
    key='return_value'  # Explicit key
)
```

### 2.5 TaskFlow API (Enhanced in 3.x)

```python
# TaskFlow API is STRONGLY RECOMMENDED in Airflow 3.x
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    schedule='@daily',
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['etl', 'taskflow'],
)
def my_etl_dag():
    
    @task
    def extract():
        """Extract data from source"""
        return {'records': 1000}
    
    @task
    def transform(data: dict):
        """Transform data"""
        data['processed'] = True
        return data
    
    @task
    def load(data: dict):
        """Load to destination"""
        print(f"Loading {data['records']} records")
    
    # Define dependencies vá»›i >>
    data = extract()
    transformed = transform(data)
    load(transformed)

# Instantiate DAG
my_etl_dag()
```

**Why TaskFlow?**
- Type hints work better
- Automatic XCom handling
- Cleaner code
- Better IDE support

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸ“¦ PART 3: PROVIDER PACKAGE VERSIONS

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### 3.1 Version Compatibility Matrix

```
Provider Package Versions for Airflow 3.1.5:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

apache-airflow-providers-apache-spark:
â”œâ”€ Airflow 2.x: 4.6.0
â””â”€ Airflow 3.x: 4.10.1 â† Updated

apache-airflow-providers-amazon:
â”œâ”€ Airflow 2.x: 8.16.0
â””â”€ Airflow 3.x: 10.2.0 â† Major version bump!

apache-airflow-providers-http:
â”œâ”€ Airflow 2.x: 4.7.0
â””â”€ Airflow 3.x: 5.1.0

apache-airflow-providers-postgres:
â”œâ”€ Airflow 2.x: 5.9.0
â””â”€ Airflow 3.x: 6.1.0
```

**IMPORTANT**: Provider versions â‰¥10.x ONLY work with Airflow 3.x!

### 3.2 Install Commands

```bash
# For Airflow 3.1.5:
pip install apache-airflow-providers-apache-spark==4.10.1
pip install apache-airflow-providers-amazon==10.2.0
pip install apache-airflow-providers-http==5.1.0

# Compatibility check:
airflow providers list  # Shows installed providers
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸ¯ PART 4: NEW FEATURES TO LEVERAGE

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### 4.1 Dataset v2 - Data-Driven Scheduling

```python
from airflow.datasets import Dataset
from airflow.decorators import dag, task
from datetime import datetime

# Define datasets
user_events = Dataset("s3://lakehouse/user-events/")
recommendations = Dataset("s3://lakehouse/recommendations/")

@dag(
    schedule=[user_events],  # Trigger when user_events updated!
    start_date=datetime(2025, 1, 1),
    catchup=False,
)
def compute_recommendations():
    
    @task(outlets=[recommendations])  # Marks dataset as updated
    def generate_recs():
        # ... compute recommendations
        return "Done"
    
    generate_recs()

compute_recommendations()
```

**Why use Datasets?**
- Trigger DAGs based on data availability (not just time)
- Better dependency tracking
- Automatic lineage visualization in UI

### 4.2 Dynamic Task Mapping (Improved)

```python
from airflow.decorators import dag, task

@dag(schedule='@daily', start_date=datetime(2025, 1, 1))
def process_movies():
    
    @task
    def get_movie_ids():
        return ['m001', 'm002', 'm003', 'm004', 'm005']
    
    @task
    def process_movie(movie_id: str):
        print(f"Processing {movie_id}")
        return f"{movie_id}_processed"
    
    # Map over list - creates N tasks dynamically!
    movie_ids = get_movie_ids()
    process_movie.expand(movie_id=movie_ids)

process_movies()
```

**Result**: Creates 5 parallel tasks automatically!

### 4.3 OpenTelemetry Integration

```python
# Airflow 3.x has built-in OTEL support!

# In airflow.cfg or env vars:
[metrics]
otel_on = True
otel_host = localhost
otel_port = 4318

# Automatically exports:
# - Task duration
# - DAG run metrics
# - Scheduler performance
# - Database query times
```

**Benefits**: Can integrate with Prometheus, Grafana, Jaeger immediately!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## âš ï¸ PART 5: GOTCHAS & TROUBLESHOOTING

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### 5.1 Common Errors After Upgrade

**Error 1**: `ImportError: cannot import name 'SubDagOperator'`
```
Solution: Replace with TaskGroup
See section 2.1 above
```

**Error 2**: `Provider version mismatch`
```
Error message:
"Provider apache-airflow-providers-amazon version 8.16.0 
 is not compatible with Airflow 3.x"

Solution:
pip install --upgrade apache-airflow-providers-amazon==10.2.0
```

**Error 3**: `Database migration failed`
```
Error: Alembic migration error

Solution:
1. Backup database first!
2. Run: airflow db migrate
3. If fails, check compatibility:
   airflow db check-migrations
```

**Error 4**: `RBAC configuration deprecated`
```
Warning: AIRFLOW__WEBSERVER__RBAC is deprecated

Solution:
Remove from docker-compose.yaml
(RBAC is always enabled in 3.x)
```

### 5.2 Performance Tuning for 3.x

```yaml
# Recommended settings for Airflow 3.x:
environment:
  # Scheduler optimization
  - AIRFLOW__SCHEDULER__USE_JOB_SCHEDULE=True
  - AIRFLOW__SCHEDULER__MAX_TIS_PER_QUERY=512
  - AIRFLOW__SCHEDULER__PARSING_PROCESSES=2
  
  # Database connection pool
  - AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE=10
  - AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW=20
  
  # Task execution
  - AIRFLOW__CORE__PARALLELISM=32
  - AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=16
  - AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG=16
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸ“ PART 6: MIGRATION CHECKLIST

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

### Phase 1: Pre-Migration (Testing)
```
â˜ Backup Airflow metadata database
â˜ Export current connections: airflow connections export
â˜ Export variables: airflow variables export
â˜ Test DAGs in Airflow 3.x dev environment
â˜ Update provider packages
â˜ Check for deprecated operators
```

### Phase 2: Docker Setup
```
â˜ Update Dockerfile to use 3.1.5 base image
â˜ Update provider package versions in requirements
â˜ Update docker-compose environment variables
â˜ Remove deprecated RBAC settings
â˜ Add new security settings
```

### Phase 3: Code Updates
```
â˜ Replace SubDagOperator with TaskGroup
â˜ Update execution_date â†’ logical_date
â˜ Update schedule_interval â†’ schedule
â˜ Add type hints to TaskFlow tasks
â˜ Test all custom operators/sensors
```

### Phase 4: Deployment
```
â˜ Stop Airflow 2.x containers
â˜ Run database migration: airflow db migrate
â˜ Start Airflow 3.x containers
â˜ Import connections & variables
â˜ Verify DAGs parse correctly
â˜ Run test DAG executions
â˜ Monitor logs for warnings
```

### Phase 5: Post-Migration
```
â˜ Enable OpenTelemetry metrics
â˜ Update monitoring dashboards
â˜ Update documentation
â˜ Train team on new UI features
â˜ Celebrate! ğŸ‰
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸš€ QUICK START - Minimal Migration

Náº¿u anh muá»‘n upgrade NHANH nháº¥t mÃ  minimal risk:

```bash
# Step 1: Update Dockerfile
# (Use the new Dockerfile I created above)

# Step 2: Update only critical env vars in docker-compose:
environment:
  - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=False
  - AIRFLOW__SCHEDULER__USE_JOB_SCHEDULE=True
  
# Remove this line:
# - AIRFLOW__WEBSERVER__RBAC=True

# Step 3: Rebuild and start:
docker compose -f docker-compose.unified.yaml build airflow-webserver airflow-scheduler
docker compose -f docker-compose.unified.yaml up -d

# Step 4: Check logs:
docker logs -f airflow-webserver
docker logs -f airflow-scheduler

# Step 5: Access UI:
http://localhost:8080
```

**Expected outcome**: Airflow 3.x running, most 2.x DAGs work unchanged!

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸ“š RESOURCES

- Official Migration Guide: https://airflow.apache.org/docs/apache-airflow/3.1.5/migrations/
- Provider Compatibility: https://airflow.apache.org/docs/apache-airflow-providers/
- Release Notes: https://airflow.apache.org/docs/apache-airflow/3.1.5/release_notes.html
- Slack Community: https://apache-airflow.slack.com/

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Anh Long Æ¡i, Ä‘Ã³ lÃ  toÃ n bá»™ migration guide! 

**Key takeaway**: Airflow 3.x is MUCH better than 2.x vÃ  migration khÃ´ng khÃ³ láº¯m. 
Äa sá»‘ code cÃ³ thá»ƒ giá»¯ nguyÃªn, chá»‰ cáº§n:
1. Update Docker base image
2. Fix a few env vars
3. Replace SubDagOperator náº¿u cÃ³ dÃ¹ng
4. Update provider versions

Anh muá»‘n tÃ´i giáº£i thÃ­ch thÃªm pháº§n nÃ o khÃ´ng? ğŸš€